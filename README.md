# Multi-AI ポモドーロTODOアプリ 実験プロジェクト

## プロジェクト概要

本プロジェクトは、複数のAI(Claude、Gemini、Codex、Cursor、Amp、Droid、Qwen)の協調開発と個別開発を比較検証する実験的な取り組みです。同一の要件定義に基づいてポモドーロTODOアプリを実装し、各AIの設計思想、実装品質、セキュリティ対策などを多角的に評価しています。

この実験は学術的な興味から実施したものであり、完璧な評価を目指したものではありません。各AIの特性を理解し、適材適所での活用を検討するための一つの視点として、知見を共有させていただきます。

## 実験の目的

- 複数AIの協調開発(Multi-AI)と単独AI開発の品質比較
- 各AIの設計思想と実装パターンの可視化
- セキュリティ対策(特にXSS)やパフォーマンス最適化の実装状況の確認
- AI自身の自己レビュー能力(メタ認知)の測定

## 実施した実験

### 実験1: 共通計画による実装力の比較

**概要**: Multi-AIが作成した共通の実装計画書を用いて、7つのAIが同じポモドーロTODOアプリを実装。実装品質を100点満点で評価しました。

**主な発見**:
- 上位: Droid(86.7点)、Multi-AI(81.7点)、Codex(78.3点)
- 2つのAI(Claude、Gemini)の実装にXSS脆弱性が存在
- DocumentFragment使用の有無でパフォーマンスに約3倍の差
- Multi-AIは2位に留まり、単独AIでも優れた成果を出せることを確認

**課題**: Multi-AIが計画を作成したため、Multi-AI有利のバイアスが存在する可能性があります。

### 実験2-1: 独立計画による計画力の評価

**概要**: 各AIが独自に実装計画を作成し、設計思想や優先順位判断を含む「計画力」を評価しました(20点満点)。

**主な発見**:
- 上位: Droid(95点)、Multi-AI(90点)、Codex(85点)
- Geminiは唯一700行制約を達成(616行)
- 計画の詳細さは必ずしも実装品質と比例しない
- 各AIの個性が明確化: Droidのエンタープライズ志向、Geminiのミニマリズム等

**洞察**: 制約の解釈がAIごとに大きく異なることが判明しました。

### 実験2-2: 計画に基づく実装力の評価

**概要**: 実験2-1で各AIが作成した計画を元に実装し、「計画と実装の整合性」を評価しました(100点満点)。

**主な発見**:
- 上位: Multi-AI(92点)、Droid(88点)、Codex(82点)
- 計画と実装のギャップ: Droid(8%)、Qwen(45%)
- XSS対策実装率: 実験1(71%) → 実験2(25%)に後退
- 独立計画では品質基準が統一されにくい傾向

**重要な教訓**: 詳細な計画(Qwen: 332行)でも実装ギャップが大きくなる場合があり、適切な粒度とチェックリストが重要です。

### 実験3: テストコード品質の比較

**概要**: 8つのAI実装に対してJestベースのテストコードを生成させ、7AIによる協調評価で品質を測定しました(100点満点)。

**主な発見**:
- 上位: Multi-AI(95点)、Claude(92点)、Amp(88点)
- Multi-AIは中央集約Mock管理とUnit/Integration完全分離を実現
- Claudeは完璧なBDD記法とトレーサビリティID体系を実装
- Ampはクラスベース設計との親和性で高評価

**評価軸**:
- コード品質(30点): 可読性、保守性、DRY原則
- テスト網羅性(25点): カバレッジ、境界値、エッジケース
- 設計思想(20点): アーキテクチャ、モジュール性
- 実用性(15点): 学習コスト、DX、実行速度
- エラーハンドリング(10点): 異常系テスト、エラー体系化

**重要な発見**: Multi-AI協調により、単一AIを上回るテスト品質を実現できることを実証しました。

### 実験4: 自己レビュー能力の測定

**概要**: 各AIに自分の実装を自己レビューさせ、メタ認知能力を評価しました。CodeRabbitとClaude Securityによる客観評価も実施。

**主な発見**:
- 上位: Cursor(94.0点)、Gemini/Droid/Qwen(84.0点)
- Cursorは7件の具体的な改善提案(他は0-2件)
- 客観評価では全AIが満点(基本品質は全て達成)
- 自己批判的なAIほど実際のコード品質が高い傾向

**重要な洞察**: 「完璧なコードを書くAI」より「自分の不完全さを認識し改善し続けるAI」の方が、長期的には高品質なコードを生み出す可能性があります。

## 主要な発見のまとめ

### セキュリティについて

- 実験1: 7つのAIのうち2つ(Claude、Gemini)にXSS脆弱性
- 実験2: 独立計画ではXSS対策実装率が71%→25%に後退
- 教訓: AIが生成したコードでも必ずセキュリティレビューが必要

### パフォーマンスについて

- DocumentFragment使用で約3倍の体感速度向上
- タイムスタンプベースのタイマー実装が精度向上に寄与
- 実装率: 実験1(57%)、実験2(50%)

### Multi-AI協調開発について

- セキュリティ・アクセシビリティで明確な効果
- 開発時間は約+80%、意思決定のオーバーヘッドあり
- 常に最善とは限らず、適材適所での活用が重要

### 計画と実装の関係

- 計画の詳細さと実装品質は必ずしも比例しない
- 適切な粒度(100-150行、5-10フェーズ)が重要
- 必須項目の明確化と優先順位判断が成否を分ける

### テストコード品質について

- 実験3: Multi-AIが95点で1位、単一AIを上回る品質を実証
- 中央集約Mock管理とUnit/Integration完全分離が効果的
- BDD記法(Given/When/Then)がトレーサビリティ向上に寄与
- 学習曲線: Gemini(初心者向け) < Claude(中級者向け) < Multi-AI(上級者向け)
- 教訓: プロジェクトのコンテキストに合わせたテストパターン選択が重要

## 実験の限界と今後の課題

本実験にはいくつかの限界があることを認識しています:

- **評価の主観性**: 評価者(Claude、および実験3では7AI協調評価)によるバイアスの可能性
- **サンプルサイズ**: 1つのアプリケーション、8つのAIでの検証のみ
- **時点の制約**: 2025年10月時点のAI能力に基づく評価
- **動作検証の不足**: 一部の実装で実際のブラウザ動作確認が不十分
- **実験1のバイアス**: Multi-AIが計画作成したことによる潜在的な有利性
- **実験3の評価方法**: 7AI協調評価の客観性・再現性の検証が不十分

今後は以下の改善を検討しています:

- 複数の評価者による相互評価
- より多様なプロジェクトタイプでの検証
- 自動テストツール(Lighthouse、OWASP ZAP等)の活用
- 長期的な保守性の評価

## 注意事項

:::warning
本ツールは個人ユース向けの試験的実装です。以下の点にご注意ください:

- 各サービス(Claude、Gemini等)の利用規約を遵守してください
- 本番環境での使用は推奨しません
- セキュリティレビューを実施した上でご利用ください
- 自己責任でのご利用をお願いいたします
:::

## 関連リポジトリ

- [multi-ai-orchestrium](https://github.com/CaCC-Lab/multi-ai-orchestrium) - Multi-AI協調開発フレームワーク

## ライセンス

MIT License

## 謝辞

本プロジェクトは、複数のAI技術の特性理解を目的とした実験的な取り組みです。完璧な評価を目指したものではなく、一つの視点として知見を共有させていただいております。

各AIの開発元(Anthropic、Google、OpenAI、Factory AI等)の技術に敬意を表し、それぞれの強みを理解し適切に活用することの重要性を学ばせていただきました。

また、本実験の記事作成にはClaude(Anthropic)を活用させていただいております。必要に応じて追加の確認や調査を推奨いたします。

---

## オープンソースプロジェクト

- **[Vibe Logger](https://github.com/fladdict/vibe-logger)** - AI用構造化ロギング
  - AI向けログフォーマット（human_note、ai_todo、ai_intent）
  - 各AIラッパーのログ実装に採用

- **[kinopeee/cursorrules](https://github.com/kinopeee/cursorrules)** - Cursor AIの効果的な活用ルールとベストプラクティス集
  - タスク分類システム（🟢🟡🔴）のアイデア
  - AGENTS.mdの設計基盤

## 登場AIツール

- [**Claude Code**](https://docs.claude.com/ja/docs/claude-code/overview)
- [**Gemini CLI**](https://github.com/google-gemini/gemini-cli)
- [**Qwen Code**](https://github.com/QwenLM/qwen-code)
- [**Codex CLI**](https://developers.openai.com/codex/cli/)
- [**Cursor CLI**](https://cursor.com/ja/docs/cli/overview)
- [**CodeRabbit CLI**](https://www.coderabbit.ai/ja/cli)
- [**Amp**](https://ampcode.com/manual)
- [**Droid CLI**](https://docs.factory.ai/cli/getting-started/quickstart)
